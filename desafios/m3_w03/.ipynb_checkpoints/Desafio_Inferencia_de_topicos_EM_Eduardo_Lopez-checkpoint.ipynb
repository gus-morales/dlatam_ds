{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre Alumno: Eduardo López\n",
    "### Generación :  G10\n",
    "### Profesores : Heriberto Briceño - Gianina Salomó \n",
    "### Fecha : Miercoles 22 de enero 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Preparar el ambiente de trabajo\n",
    "* Importe los módulos numpy, pandas, matplotlib, seaborn, glob y os siguiendo lasbuenas prácticas. Los últimos dos módulos permitirán realizar la importación de múltiples archivos dentro de la carpeta dump.\n",
    "* Para ello genere un objeto que guarde en una lista todos los archivos alojados en dump utilizando glob.glob y os.getcwd() para extraer las rutas absolutas. Posteriormente genere un objeto pd.DataFrame que contenga todos los csv.\n",
    "* Asegúrese de eliminar la columna Unnamed: 0 que se genera por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import glob \n",
    "import os \n",
    "import warnings\n",
    "from matplotlib import rcParams\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "rute_dump = glob.glob(\"dump/*.csv\")\n",
    "\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dump\\\\anthrax_scrape.csv',\n",
       " 'dump\\\\a_tribe_called_quest_scrape.csv',\n",
       " 'dump\\\\black_star_scrape.csv',\n",
       " 'dump\\\\bob_dylan_scrape.csv',\n",
       " 'dump\\\\britney_spears_scrape.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rute_dump[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = []\n",
    "\n",
    "for filename in rute_dump:\n",
    "    df = pd.read_csv(filename, index_col = None, header = 0)\n",
    "    list_df.append(df)\n",
    "\n",
    "df = pd.concat(list_df, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Deathrider</td>\n",
       "      <td>Riding hard, high in the saddle \\n Winged stee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Metal Thrashing Mad</td>\n",
       "      <td>Racing down the road \\n In a street machine of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>I'm Eighteen</td>\n",
       "      <td>Lines form on my face and hands \\n Lines form ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Panic</td>\n",
       "      <td>Move it to the front \\n Reaching for the light...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Subjugator</td>\n",
       "      <td>Out in the streets \\n We're fighting tonight \\...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9484</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>Thief, You've Taken All That Was Me</td>\n",
       "      <td>Thief, of silent dreams \\n Of golden scenes \\n...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9485</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>We Are All In Love</td>\n",
       "      <td>When your out with your friends \\n In your new...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9486</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>We Go Together</td>\n",
       "      <td>We go together \\n And that's to stay \\n And in...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9487</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>What Is This I Find?</td>\n",
       "      <td>[Jonas]: \\n What is this I find? \\n [Wuan]: \\n...</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9488</th>\n",
       "      <td>Weezer</td>\n",
       "      <td>rock</td>\n",
       "      <td>Zep Jamb</td>\n",
       "      <td>Alright \\n Say baby \\n Love me \\n Lay your mon...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9489 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1                                    2  \\\n",
       "0     Anthrax  metal                           Deathrider   \n",
       "1     Anthrax  metal                  Metal Thrashing Mad   \n",
       "2     Anthrax  metal                         I'm Eighteen   \n",
       "3     Anthrax  metal                                Panic   \n",
       "4     Anthrax  metal                           Subjugator   \n",
       "...       ...    ...                                  ...   \n",
       "9484   Weezer   rock  Thief, You've Taken All That Was Me   \n",
       "9485   Weezer   rock                   We Are All In Love   \n",
       "9486   Weezer   rock                       We Go Together   \n",
       "9487   Weezer   rock                 What Is This I Find?   \n",
       "9488   Weezer   rock                             Zep Jamb   \n",
       "\n",
       "                                                      3 Unnamed: 0  \n",
       "0     Riding hard, high in the saddle \\n Winged stee...          0  \n",
       "1     Racing down the road \\n In a street machine of...          1  \n",
       "2     Lines form on my face and hands \\n Lines form ...          2  \n",
       "3     Move it to the front \\n Reaching for the light...          3  \n",
       "4     Out in the streets \\n We're fighting tonight \\...          4  \n",
       "...                                                 ...        ...  \n",
       "9484  Thief, of silent dreams \\n Of golden scenes \\n...        246  \n",
       "9485  When your out with your friends \\n In your new...        247  \n",
       "9486  We go together \\n And that's to stay \\n And in...        248  \n",
       "9487  [Jonas]: \\n What is this I find? \\n [Wuan]: \\n...        249  \n",
       "9488  Alright \\n Say baby \\n Love me \\n Lay your mon...        250  \n",
       "\n",
       "[9489 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"0\":\"artista\", \"1\":\"genero\", \"2\":\"cancion\", \"3\":\"letra\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9489 entries, 0 to 9488\n",
      "Data columns (total 4 columns):\n",
      "artista    9489 non-null object\n",
      "genero     9489 non-null object\n",
      "cancion    9489 non-null object\n",
      "letra      9489 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 296.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existen valores perdidos (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artista</th>\n",
       "      <th>genero</th>\n",
       "      <th>cancion</th>\n",
       "      <th>letra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Deathrider</td>\n",
       "      <td>Riding hard, high in the saddle \\n Winged stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Metal Thrashing Mad</td>\n",
       "      <td>Racing down the road \\n In a street machine of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>I'm Eighteen</td>\n",
       "      <td>Lines form on my face and hands \\n Lines form ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Panic</td>\n",
       "      <td>Move it to the front \\n Reaching for the light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Subjugator</td>\n",
       "      <td>Out in the streets \\n We're fighting tonight \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artista genero              cancion  \\\n",
       "0  Anthrax  metal           Deathrider   \n",
       "1  Anthrax  metal  Metal Thrashing Mad   \n",
       "2  Anthrax  metal         I'm Eighteen   \n",
       "3  Anthrax  metal                Panic   \n",
       "4  Anthrax  metal           Subjugator   \n",
       "\n",
       "                                               letra  \n",
       "0  Riding hard, high in the saddle \\n Winged stee...  \n",
       "1  Racing down the road \\n In a street machine of...  \n",
       "2  Lines form on my face and hands \\n Lines form ...  \n",
       "3  Move it to the front \\n Reaching for the light...  \n",
       "4  Out in the streets \\n We're fighting tonight \\...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Matriz de ocurrencias\n",
    "* Importe la clase CountVectorizer dentro de los módulos feature_extraction.text de la librería sklearn.\n",
    "* Aplique la clase para extraer las 5000 palabras más repetidas en toda la base de datos.\n",
    "* Con la clase inicializada, incorpore las letras con el método fit_transform y guarde los resultados en un nuevo objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instanciamos un objeto\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "# Implementamos los pasos fit y transform\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df[\"letra\"])\n",
    "# Extraemos tokens (palabras)\n",
    "words = count_vectorizer.get_feature_names()\n",
    "# extraemos frecuencia\n",
    "words_freq = count_vectorizer_fit.toarray().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame({\"words\":words, \"frequency\":words_freq})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las 5000 palabras mas repetidas en canciones en todo el data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24950</th>\n",
       "      <td>like</td>\n",
       "      <td>19629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12859</th>\n",
       "      <td>don</td>\n",
       "      <td>17398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23856</th>\n",
       "      <td>know</td>\n",
       "      <td>14962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18439</th>\n",
       "      <td>got</td>\n",
       "      <td>14171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23287</th>\n",
       "      <td>just</td>\n",
       "      <td>13978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>crawled</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>absolute</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43667</th>\n",
       "      <td>thursday</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45019</th>\n",
       "      <td>tweedle</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13111</th>\n",
       "      <td>dragging</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  frequency\n",
       "24950      like      19629\n",
       "12859       don      17398\n",
       "23856      know      14962\n",
       "18439       got      14171\n",
       "23287      just      13978\n",
       "...         ...        ...\n",
       "10010   crawled         31\n",
       "630    absolute         31\n",
       "43667  thursday         31\n",
       "45019   tweedle         31\n",
       "13111  dragging         31\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.sort_values(\"frequency\", ascending = False).head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Entrenamiento del Modelo\n",
    "* Importe sklearn.decomposition.LatentDirichletAllocation y sklearn.model_selection.GridSearchCV. \n",
    "* Genere una búsqueda de grilla con los siguientes hiperparámetros:\n",
    "    * n_components: [5, 10, 15].\n",
    "    * learning_decay: [0.7, 0.5].\n",
    "* Entrene la búsqueda de grilla con las letras en un formato vectorizado con CountVectorizer. \n",
    "* Reporte brevemente cuál es la mejor combinación de hiperparámetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params = {\"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "          \"gamma\": [0.0000001, 0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "search_params = {\"n_components\": [5, 10, 15], \"learning_decay\": [0.7, 0.5]}\n",
    "\n",
    "pipe_model = Pipeline([(\"count_vectorizer\", CountVectorizer(stop_words = \"english\")),\n",
    "                       (\"lda\", LatentDirichletAllocation(n_components, learning_decay)),\n",
    "                       (\"grid\", GridSearchCV(lda,\n",
    "                                             param_grid = params, cv = 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros de LDA\n",
    "search_params = {\"n_components\": [5, 10, 15], \"learning_decay\": [0.7, 0.5]}\n",
    "# Tokenizacion de letras de canciones\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df[\"letra\"])\n",
    "# Entrenamiento del modelo\n",
    "lda_model = LatentDirichletAllocation(max_iter=10,               \n",
    "                                    random_state = 63,       \n",
    "                                    batch_size = 128,         \n",
    "                                    evaluate_every = -1,    \n",
    "                                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(lda_model, param_grid = search_params, cv = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='batch',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=-1,\n",
       "                                                 perp_tol=0.1, random_state=63,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'learning_decay': [0.7, 0.5],\n",
       "                         'n_components': [5, 10, 15]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor combinación de hiperparametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.7, 'n_components': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4 : Inferencia e Identificación de Tópicos\n",
    "* En base a la mejor combinación de hiperparámetros, entrene el modelo con la matriz de atributos de las letras.\n",
    "* Para identificar de qué se trata cada tópico, necesitamos identificar las principales 15 palabras asociadas con éste. Puede implementar la siguiente línea de código para identificar las principales palabras en un tópico.\n",
    "* Comente a qué tópicos está asociada cada clase inferida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "lda_model = LatentDirichletAllocation(n_components = 5,\n",
    "                                      learning_decay = 0.7,\n",
    "                                      max_iter = 10,               \n",
    "                                      random_state = 63,       \n",
    "                                      batch_size = 128,         \n",
    "                                      evaluate_every = -1,    \n",
    "                                      n_jobs = -1)\n",
    "\n",
    "lda_model_fit = lda_model.fit(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tópico: 1\n",
      "love don know oh just yeah ll baby like ve time got let come want\n",
      "tópico: 2\n",
      "like don got just cause ain know shit say fuck em wanna man make ll\n",
      "tópico: 3\n",
      "like nigga yo got shit know man niggas don ya ain yeah fuck em just\n",
      "tópico: 4\n",
      "la death god life blood die world dead black na eyes soul hell light man\n",
      "tópico: 5\n",
      "like yeah da ma doo got ha ah money man cause big dre ass don\n"
     ]
    }
   ],
   "source": [
    "# mediante .components_ podemos extraer una matriz \n",
    "# que entrega las distribución de palabras por cada tópico.\n",
    "for topic_id, topic_name in enumerate(lda_model_fit.components_):\n",
    "    # para cada tópico \n",
    "    print(\"tópico: {}\".format(topic_id+1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud\n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([count_vectorizer.get_feature_names()[i] for i in topic_name.argsort()[:-15-1: -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Identificación de probabilidades\n",
    "* En base a la información generada, es posible identificar cuales van a ser los géneros más probables de ocurrir para un artista.\n",
    "* Para ello necesitamos guardar la probabilidad de cada canción en nuestra base de datos original. Podemos implementar esto de la siguiente manera.\n",
    "\n",
    "* Genere una matriz de correlaciones entre la probabilidad de tópicos inferidos.Comente brevemente cuales son las principales asociaciones existentes.\n",
    "* Con esta nueva base de datos, identifique las probabilidades de pertenencia para un artista específico. \n",
    "Grafique la distribución de las probabilidades para algún artista en específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-696ebb540402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mconcatenated_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics_for_each_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# argmax en la matriz de tópicos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mconcatenated_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'highest_topic'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'docs_topics' is not defined"
     ]
    }
   ],
   "source": [
    "# generamos una transformación de los datos a distribución de tópico por palabra en el documento\n",
    "fit_best_lda = lda_model.transform(count_vectorizer_fit)\n",
    "# estra transformación la podemos coercionar a un dataframe de la siguiente manera\n",
    "topics_for_each_doc = pd.DataFrame(\n",
    "    # pasamos esta matriz y la redondeamos en 3 decimales\n",
    "    np.round(fit_best_lda, 3),\n",
    "    # agregamos un índice\n",
    "    index = df.index)\n",
    "#agregamos identificadores de columna \n",
    "topics_for_each_doc.columns = list(map(lambda x: \"T: {}\".format(x), range(1,lda_model.n_components+1)))\n",
    "# concatenamos las probabilidades de tópico por documento a nuestra matriz original \n",
    "concatenated_df = pd.concat([df, topics_for_each_doc], axis=1)\n",
    "# argmax en la matriz de tópicos\n",
    "concatenated_df['highest_topic'] = np.argmax(docs_topics.values, axis = 1) +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
